{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306aa66b-e491-43b0-be41-5aadcfe514aa",
   "metadata": {},
   "source": [
    "# parsing published_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4164b-bfc5-4067-9252-15b8db630ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [\"3 days ago\", \"2 weeks ago\", \"1 month ago\", \"13 minutes ago\", \"4 hours ago\"]:\n",
    "    print(s, \"->\", mvf.parse_date(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e547e4e-b160-449b-bb2e-7b46f4ed7763",
   "metadata": {},
   "source": [
    "# main old 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5ce31-ec04-4e4a-b9ef-5ee62ef8bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "from difflib import get_close_matches\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "import func_bit as mvf  # Project-specific functions\n",
    "import importlib\n",
    "importlib.reload(mvf)\n",
    "\n",
    "# Загрузка переменных окружения из .env\n",
    "load_dotenv()\n",
    "\n",
    "# Получение API-ключа\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Загрузка переменных и словарей\n",
    "from config_bit import (\n",
    "    DATA_FOLDER, REPORT_FOLDER, RESULT_FOLDER, \n",
    "    SCRAPING_DATE, DEDUPLICATION_MODE, KEEP,\n",
    "    EMPLOYMENT_TYPES, WORK_TYPES,\n",
    "    HARD_SKILLS, SOFT_SKILLS, LANG_PATTERNS, LANG_LEVELS,LANG_LEVEL_DESCRIPTIONS,\n",
    "    GRADE_KEYWORDS, DIRECTION_PRIORITY, AI_SPETIALIST_TERMS,\n",
    "    CURRENCY_RATES, PERIOD_MULTIPLIERS\n",
    ")\n",
    "\n",
    "# Создаем папки, если их нет\n",
    "REPORT_FOLDER.mkdir(exist_ok=True)\n",
    "RESULT_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "# Установка опций просмотра\n",
    "pd.set_option(\"display.max_rows\", 20) \n",
    "pd.set_option(\"display.max_columns\", 50) \n",
    "\n",
    "\n",
    "# ========================== 1. Загружаем Excel напрямую в DataFrame ===\n",
    "file_path = os.path.join(\"data\", \"dataset_linkedin-jobs-scraper-no-login-required_2025-12-14_09-12-49-010.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"Начальное количество записей:\", len(df))\n",
    "\n",
    "# ========================== 2. Удаляем записи старше 6 месяцев ===\n",
    "# дата отсечения: 6 месяцев назад от даты скрапинга\n",
    "six_months_ago = SCRAPING_DATE - timedelta(days=180)\n",
    "\n",
    "# получаем реальную дату публикации\n",
    "df[\"posted_at\"] = df[\"published_at\"].apply(mvf.parse_date)\n",
    "\n",
    "# фильтруем только свежие записи > 6 мес\n",
    "df = df[df[\"posted_at\"].notna() & (df[\"posted_at\"] >= six_months_ago)]\n",
    "print(\"После удаления старше 6 месяцев:\", len(df))\n",
    "\n",
    "\n",
    "# ========================== 3. Удаляем полные дубликаты ===\n",
    "df = df.drop_duplicates()\n",
    "print(\"После удаления полных дубликатов:\", len(df))\n",
    "\n",
    "# ========================== 4. Удаляем дубликаты по ключам job_title+company_name+location ===\n",
    "df = df.drop_duplicates(\n",
    "     subset=[\"job_title\", \"company_name\", \"location\"],\n",
    "     keep=\"first\"\n",
    ")\n",
    "print(\"После удаления дубликатов по ключам:\", len(df))\n",
    "\n",
    "# ========================== 5. Убираем строки со значениями [\"Mid-Senior level\", \"Not Applicable\" в \"seniority_level\"\n",
    "df_clean = df[~df[\"seniority_level\"].isin([\"Mid-Senior level\", \"Not Applicable\"])]\n",
    "print(\"Количество строк entry level после фильтрации:\", len(df_clean))\n",
    "\n",
    "# ========================== 6. чистим job_title в job_title_clean\n",
    "df_clean = df_clean.copy()\n",
    "# применяем функцию очистки к колонке job_title\n",
    "df_clean.loc[:, \"job_title_clean\"] = mvf.clean_job_titles(df_clean[\"job_title\"].tolist())\n",
    "\n",
    "# ========================== 7. переименуем колонки для дальнейшего использования готовых функций\n",
    "df_clean = df_clean.rename(columns={\n",
    "    'company_name': 'company',\n",
    "    'description_text': 'description',\n",
    "    'employment_type' : 'contractType',\n",
    "    'seniority_level' : 'experienceLevel'\n",
    "})\n",
    "\n",
    "display(df_clean)\n",
    "\n",
    "# ========================== 8. Сохраняем результат в Excel preclean.xlsx ===\n",
    "output_path = \"preclean.xlsx\"\n",
    "df_clean.to_excel(output_path, index=False)\n",
    "print(f\"Файл сохранён: {output_path}\")\n",
    "print(\"Количество строк в итоговом датафрейме:\", len(df_clean))\n",
    "\n",
    "# # ========================== 9. Добавляем новый столбец с классификацией\n",
    "# df = df_clean.copy()\n",
    "# df.loc[:, \"direction\"] = df[\"description_text\"].apply(\n",
    "#     lambda x: mvf.classify_direction(x, AI_SPETIALIST_TERMS)\n",
    "# )\n",
    "\n",
    "# # Проверим распределение по направлениям\n",
    "# print(\"-----------------------------\")\n",
    "# print(df[\"direction\"].value_counts())\n",
    "\n",
    "# ========================== 8 Извлечение при помощи AI из description directions, hard_skills, soft_skills и salary\n",
    "# tqdm.pandas(desc=\"Анализ description_text...\")\n",
    "# df_clean[[\"directions_AI\", \"hard_skills\", \"soft_skills\", \"salary_range\"]] = df_clean.progress_apply(\n",
    "#     lambda row: pd.Series(\n",
    "#         mvf.analyze_description(row[\"description_text\"], row[\"salary_range\"], client)\n",
    "#     ),\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "\n",
    "# print(df_clean[\"direction\"].value_counts())\n",
    "#print(df_clean[[\"job_title\", \"hard_skills\", \"soft_skills\", \"salary_range\"]])\n",
    "\n",
    "## ========================== 9. Приведение ЗП к евро в год\n",
    "# df_salary = mvf.normalize_salary_field(df_clean, salary_col=\"salary_range\")\n",
    "\n",
    "## ========================== 10. Нормализация locations - выделение города, земли, страны\n",
    "# df_pre = mvf.clean_and_enrich_germany_locations(df_salary)\n",
    "\n",
    "# # ========================== 11.. Сохранение результата pre-cleaned\n",
    "# df_pre_path = RESULT_FOLDER / \"03_merged_jobs_pre-clean.xlsx\"\n",
    "# df_pre.to_excel(df_pre_path, index=False)\n",
    "# print(f\"Информация после предварительного обогащения (salary, location) сохранена в файл: {df_pre_path}\")\n",
    "\n",
    "## ========================== 12. Запуск финального пайплайна на df_pre\n",
    "# #reports = mvf.run_ecommerce_pipeline(df_pre, verbose=True, logger=logger)\n",
    "\n",
    "\n",
    "## ========================== 13. Запуск просмотра\n",
    "# #mvf.show_reports(reports, top_n=10)\n",
    "\n",
    "\n",
    "# # 13 Анализ по федеральным землям\n",
    "# # df_state_stats = mvf.analyze_jobs_by_state(df_clean, top_n=10)\n",
    "# # print(df_state_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23267a1d-dc74-4af7-9d8e-7570e2c4e34e",
   "metadata": {},
   "source": [
    "# main old 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc159b-8214-46df-87b8-88db629d3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "from difflib import get_close_matches\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "import func_bit as mvf  # Project-specific functions\n",
    "import importlib\n",
    "importlib.reload(mvf)\n",
    "\n",
    "# Загрузка переменных окружения из .env\n",
    "load_dotenv()\n",
    "\n",
    "# Получение API-ключа\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Загрузка переменных и словарей\n",
    "from config_bit import (\n",
    "    DATA_FOLDER, REPORT_FOLDER, RESULT_FOLDER, \n",
    "    SCRAPING_DATE, DEDUPLICATION_MODE, KEEP,\n",
    "    EMPLOYMENT_TYPES, WORK_TYPES,\n",
    "    HARD_SKILLS, SOFT_SKILLS, LANG_PATTERNS, LANG_LEVELS,LANG_LEVEL_DESCRIPTIONS,\n",
    "    GRADE_KEYWORDS, DIRECTION_PRIORITY, AI_SPETIALIST_TERMS,\n",
    "    CURRENCY_RATES, PERIOD_MULTIPLIERS\n",
    ")\n",
    "\n",
    "# Создаем папки, если их нет\n",
    "REPORT_FOLDER.mkdir(exist_ok=True)\n",
    "RESULT_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "# Установка опций просмотра\n",
    "pd.set_option(\"display.max_rows\", 20) \n",
    "pd.set_option(\"display.max_columns\", 50) \n",
    "\n",
    "\n",
    "# ========================== 1. Загружаем Excel напрямую в DataFrame ===\n",
    "file_path = os.path.join(\"data\", \"dataset_linkedin-jobs-scraper-no-login-required_2025-12-14_09-12-49-010.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"Начальное количество записей:\", len(df))\n",
    "\n",
    "# ========================== 2. Удаляем записи старше 6 месяцев ===\n",
    "# дата отсечения: 6 месяцев назад от даты скрапинга\n",
    "six_months_ago = SCRAPING_DATE - timedelta(days=180)\n",
    "\n",
    "# получаем реальную дату публикации\n",
    "df[\"posted_at\"] = df[\"published_at\"].apply(mvf.parse_date)\n",
    "\n",
    "# фильтруем только свежие записи > 6 мес\n",
    "df = df[df[\"posted_at\"].notna() & (df[\"posted_at\"] >= six_months_ago)]\n",
    "print(\"После удаления старше 6 месяцев:\", len(df))\n",
    "\n",
    "\n",
    "# ========================== 3. Удаляем полные дубликаты ===\n",
    "df = df.drop_duplicates()\n",
    "print(\"После удаления полных дубликатов:\", len(df))\n",
    "\n",
    "# ========================== 4. Удаляем дубликаты по ключам job_title+company_name+location ===\n",
    "df = df.drop_duplicates(\n",
    "     subset=[\"job_title\", \"company_name\", \"location\"],\n",
    "     keep=\"first\"\n",
    ")\n",
    "print(\"После удаления дубликатов по ключам:\", len(df))\n",
    "\n",
    "# ========================== 5. Убираем строки со значениями [\"Mid-Senior level\", \"Not Applicable\" в \"seniority_level\"\n",
    "EXCLUDE_LEVELS = [\"Mid-Senior level\", \"Not Applicable\"]\n",
    "df_clean = df[~df[\"seniority_level\"].isin(EXCLUDE_LEVELS)].copy()\n",
    "print(\"Количество строк entry level после фильтрации:\", len(df_clean))\n",
    "\n",
    "# ========================== 6. чистим job_title в job_title_clean\n",
    "df_clean = df_clean.copy()\n",
    "# применяем функцию очистки к колонке job_title\n",
    "df_clean.loc[:, \"job_title_clean\"] = mvf.clean_job_titles(df_clean[\"job_title\"].tolist())\n",
    "\n",
    "# ========================== 7. переименуем колонки для дальнейшего использования готовых функций\n",
    "df_clean = df_clean.rename(columns={\n",
    "    'company_name': 'company',\n",
    "    'description_text': 'description',\n",
    "    'employment_type' : 'contractType',\n",
    "    'seniority_level' : 'experienceLevel'\n",
    "})\n",
    "\n",
    "\n",
    "#display(df_clean)\n",
    "\n",
    "# # ========================= 8. Сохраняем результат в Excel preclean.xlsx ===\n",
    "df_clean_path = RESULT_FOLDER / \"clean_vacancies_res.xlsx\"\n",
    "df_clean.to_excel(df_clean_path, index=False)\n",
    "print(f\"Результирующий файл сохранён: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca77e0-b97d-43b6-b1bb-69de0c4bec77",
   "metadata": {},
   "source": [
    "## HARD и SOFT скилы для джунов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23a3f7-9a24-4509-8f5c-cde2a782c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_junior_skills_interactive(hard_counter, soft_counter, scraping_date, report_folder: Path, syst_name: str):\n",
    "    \"\"\"Строит одну диаграмму с переключателем HARD/SOFT skills и сохраняет в REPORT_FOLDER.\"\"\"\n",
    "\n",
    "    # преобразуем в DataFrame топ-10\n",
    "    hard_df = pd.DataFrame(hard_counter.most_common(10), columns=[\"skill\", \"count\"])\n",
    "    soft_df = pd.DataFrame(soft_counter.most_common(10), columns=[\"skill\", \"count\"])\n",
    "\n",
    "    # базовый график (по умолчанию HARD)\n",
    "    fig = px.bar(\n",
    "        hard_df,\n",
    "        x=\"skill\",\n",
    "        y=\"count\",\n",
    "        text=\"count\",\n",
    "        title=f\"ТОП HARD skills джунов ({syst_name}, за пол года от {scraping_date.strftime('%d.%m.%Y')})\",\n",
    "        labels={\"skill\": \"Навык\", \"count\": \"Количество\"},\n",
    "        color_discrete_sequence=[\"#4472C4\"]\n",
    "    )\n",
    "    fig.update_traces(textposition=\"outside\")\n",
    "\n",
    "    # кнопки переключения\n",
    "    buttons = [\n",
    "        dict(\n",
    "            label=\"HARD skills\",\n",
    "            method=\"update\",\n",
    "            args=[\n",
    "                {\"x\": [hard_df[\"skill\"]], \"y\": [hard_df[\"count\"]], \"text\": [hard_df[\"count\"]],\n",
    "                 \"marker\": {\"color\": \"#4472C4\"}, \"type\": \"bar\"},\n",
    "                {\"title\": {\"text\": f\"ТОП HARD-skills джунов ({syst_name},  за пол года от {scraping_date.strftime('%d.%m.%Y')})\",\n",
    "                           \"font\": dict(family=\"Arial Bold, sans-serif\", size=16, color=\"darkblue\")}}\n",
    "            ]\n",
    "        ),\n",
    "        dict(\n",
    "            label=\"SOFT skills\",\n",
    "            method=\"update\",\n",
    "            args=[\n",
    "                {\"x\": [soft_df[\"skill\"]], \"y\": [soft_df[\"count\"]], \"text\": [soft_df[\"count\"]],\n",
    "                 \"marker\": {\"color\": \"#70AD47\"}, \"type\": \"bar\"},\n",
    "                {\"title\": {\"text\": f\"ТОП SOFT-skills джунов ({syst_name}, за пол года от {scraping_date.strftime('%d.%m.%Y')})\",\n",
    "                           \"font\": dict(family=\"Arial Bold, sans-serif\", size=16, color=\"darkgreen\")}}\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    fig.update_layout(\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\",\n",
    "            buttons=buttons,\n",
    "            direction=\"down\",  \n",
    "            showactive=True,\n",
    "            x=1.05,\n",
    "            y=1.0,\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"top\"\n",
    "        )],\n",
    "        xaxis_tickangle=-30\n",
    "    )\n",
    "\n",
    "\n",
    "    # сохранение\n",
    "    # output_path = report_folder / \"junior_skills_interactive.html\"\n",
    "    # fig.write_html(output_path, auto_open=True)\n",
    "    # print(f\"Интерактивная диаграмма HARD/SOFT skills сохранена: {output_path}\")\n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
