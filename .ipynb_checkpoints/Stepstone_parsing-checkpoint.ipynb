{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d21804-305b-4b55-8138-8e6b687052b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1pip install requests beautifulsoup4 lxml pandas\n",
    "# !pip install tqdm\n",
    "#!pip install undetected-chromedriver selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e272bad-0451-4c40-896a-57facfb784b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è. –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:\n",
    "–°–∫—Ä–æ–ª–ª–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: –ë–æ—Ç —Ç–µ–ø–µ—Ä—å –ø—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–Ω–∏–∑ (—ç—Ç–æ —á–∞—Å—Ç–æ —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç –ø–æ–¥–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ \"—É—Å–ø–æ–∫–∞–∏–≤–∞–µ—Ç\" –∞–Ω—Ç–∏—Ñ—Ä–æ–¥).\n",
    "–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: –ï—Å–ª–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–∫—Ä–∏–ø—Ç –Ω–µ –≤—ã–∫–ª—é—á–∞–µ—Ç—Å—è, –∞ –¥–µ–ª–∞–µ—Ç –ø–∞—É–∑—É –∏ –≥—Ä–æ–º–∫–æ (—á–µ—Ä–µ–∑ print) –ø—Ä–æ—Å–∏—Ç –≤–∞—Å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä –∏ —Ä–µ—à–∏—Ç—å –∫–∞–ø—á—É –≤—Ä—É—á–Ω—É—é. –ü–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è Enter –æ–Ω –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Ä–∞–±–æ—Ç—É.\n",
    "–°–ª—É—á–∞–π–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è: –î–æ–±–∞–≤–ª–µ–Ω–∞ —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π.\n",
    "\n",
    "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫ –¥–µ–π—Å—Ç–≤–∏—é:\n",
    "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç.\n",
    "–ù–µ —Å–≤–æ—Ä–∞—á–∏–≤–∞–π—Ç–µ –æ–∫–Ω–æ –±—Ä–∞—É–∑–µ—Ä–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é (–º–æ–∂–Ω–æ –¥–µ—Ä–∂–∞—Ç—å –Ω–∞ —Ñ–æ–Ω–µ).\n",
    "–ï—Å–ª–∏ –≤ –∫–æ–Ω—Å–æ–ª–∏ –ø–æ—è–≤–∏—Ç—Å—è —Å–æ–æ–±—â–µ–Ω–∏–µ: üëâ –ü–û–ñ–ê–õ–£–ô–°–¢–ê, –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤ –±—Ä–∞—É–∑–µ—Ä!:\n",
    "–û—Ç–∫—Ä–æ–π—Ç–µ –æ—Ç–∫—Ä—ã–≤—à–µ–µ—Å—è –æ–∫–Ω–æ Chrome.\n",
    "–†–µ—à–∏—Ç–µ –ø–∞–∑–ª/–Ω–∞–∂–º–∏—Ç–µ –≥–∞–ª–æ—á–∫—É \"–Ø —á–µ–ª–æ–≤–µ–∫\".\n",
    "–î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π.\n",
    "–í–µ—Ä–Ω–∏—Ç–µ—Å—å –≤ –∫–æ–Ω—Å–æ–ª—å (PyCharm/Terminal) –∏ –Ω–∞–∂–º–∏—Ç–µ Enter.\n",
    "–°–∫—Ä–∏–ø—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Å–±–æ—Ä —Å —Ç–æ–≥–æ –∂–µ –º–µ—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c051f67-411b-4527-a080-ff924e5e4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# -------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ --------\n",
    "BASE_URL = \"https://www.stepstone.de/jobs/automation-specialist/in-deutschland\"\n",
    "OUT_CSV = \"stepstone_results_full.csv\"\n",
    "MAX_PAGES = 1000 # –°—Ç–∞–≤–∏–º —Å –∑–∞–ø–∞—Å–æ–º\n",
    "\n",
    "def get_url(page):\n",
    "    return f\"{BASE_URL}?page={page}&radius=30\"\n",
    "\n",
    "def parse_html_card(soup_article):\n",
    "    \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π HTML –∫–∞—Ä—Ç–æ—á–∫–∏.\"\"\"\n",
    "    try:\n",
    "        link_tag = soup_article.find(\"a\", href=True)\n",
    "        if not link_tag: return None\n",
    "        \n",
    "        job_url = \"https://www.stepstone.de\" + link_tag['href']\n",
    "        title = link_tag.get_text(strip=True)\n",
    "        \n",
    "        company_tag = soup_article.find(attrs={\"data-at\": \"job-item-company-name\"})\n",
    "        company = company_tag.get_text(strip=True) if company_tag else \"Unknown\"\n",
    "        \n",
    "        location_tag = soup_article.find(attrs={\"data-at\": \"job-item-location\"})\n",
    "        location = location_tag.get_text(strip=True) if location_tag else None\n",
    "        \n",
    "        date_tag = soup_article.find(attrs={\"data-at\": \"job-item-timeago\"})\n",
    "        date_posted = date_tag.get_text(strip=True) if date_tag else None\n",
    "        \n",
    "        salary_tag = soup_article.find(attrs={\"data-at\": \"job-item-salary-range\"})\n",
    "        salary = salary_tag.get_text(strip=True) if salary_tag else None\n",
    "\n",
    "        snippet_tag = soup_article.find(attrs={\"data-at\": \"job-item-snippet\"})\n",
    "        description = snippet_tag.get_text(strip=True) if snippet_tag else \"\"\n",
    "\n",
    "        return {\n",
    "            \"job_title\": title,\n",
    "            \"company\": company,\n",
    "            \"location\": location,\n",
    "            \"salary_preview\": salary,\n",
    "            \"date_posted_relative\": date_posted,\n",
    "            \"url\": job_url,\n",
    "            \"short_description\": description\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def random_scroll(driver):\n",
    "    \"\"\"–ò–º–∏—Ç–∞—Ü–∏—è —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞ —á–µ–ª–æ–≤–µ–∫–∞.\"\"\"\n",
    "    total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    for i in range(1, total_height, random.randint(300, 700)):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "    # –í–æ–∑–≤—Ä–∞—Ç –Ω–∞–≤–µ—Ä—Ö –∏–ª–∏ –∫ —Å–µ—Ä–µ–¥–∏–Ω–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "def main():\n",
    "    print(\"--- –ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ ---\")\n",
    "    options = uc.ChromeOptions()\n",
    "    # options.add_argument('--headless') # –ù–µ –≤–∫–ª—é—á–∞–µ–º headless!\n",
    "    \n",
    "    driver = uc.Chrome(options=options)\n",
    "    all_jobs = []\n",
    "    \n",
    "    # –ß—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏\n",
    "    try:\n",
    "        pbar = tqdm(range(1, MAX_PAGES + 1), unit=\"page\")\n",
    "        for page in pbar:\n",
    "            url = get_url(page)\n",
    "            driver.get(url)\n",
    "            \n",
    "            # --- –ò–º–∏—Ç–∞—Ü–∏—è —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---\n",
    "            try:\n",
    "                # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–ª–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "                # –ù–µ–º–Ω–æ–≥–æ —Å–∫—Ä–æ–ª–ª–∏–º, —á—Ç–æ–±—ã —Å–∞–π—Ç –ø–æ–¥—É–º–∞–ª, —á—Ç–æ –º—ã —á–µ–ª–æ–≤–µ–∫\n",
    "                random_scroll(driver)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # --- –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ ---\n",
    "            attempts = 0\n",
    "            articles = []\n",
    "            \n",
    "            while attempts < 2:\n",
    "                soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                articles = soup.find_all(\"article\")\n",
    "                \n",
    "                if articles:\n",
    "                    break # –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–∞–π–¥–µ–Ω—ã, –∏–¥–µ–º –¥–∞–ª—å—à–µ\n",
    "                \n",
    "                # –ï—Å–ª–∏ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–µ—Ç - –≤–æ–∑–º–æ–∂–Ω–æ –∫–∞–ø—á–∞\n",
    "                attempts += 1\n",
    "                tqdm.write(f\"‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –ø—É—Å—Ç–∞/–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞.\")\n",
    "                tqdm.write(\"üëâ –ü–û–ñ–ê–õ–£–ô–°–¢–ê, –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤ –±—Ä–∞—É–∑–µ—Ä!\")\n",
    "                tqdm.write(\"–ï—Å–ª–∏ —Ç–∞–º –∫–∞–ø—á–∞ ‚Äî —Ä–µ—à–∏—Ç–µ –µ—ë. –ï—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî –æ–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ä—É—á–Ω—É—é.\")\n",
    "                input(\"‚å®Ô∏è  –ù–∞–∂–º–∏—Ç–µ ENTER –≤ –∫–æ–Ω—Å–æ–ª–∏ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –¥–æ—Å—Ç—É–ø (–∏–ª–∏ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏)...\")\n",
    "                \n",
    "                # –ü–æ—Å–ª–µ —Ä—É—á–Ω–æ–≥–æ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—É–ø\n",
    "                soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                articles = soup.find_all(\"article\")\n",
    "            \n",
    "            if not articles:\n",
    "                tqdm.write(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page} –¥–∞–∂–µ –ø–æ—Å–ª–µ –ø–∞—É–∑—ã. –ü—Ä–æ–ø—É—Å–∫.\")\n",
    "                # –ú–æ–∂–Ω–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å break, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è —Å–æ–≤—Å–µ–º\n",
    "                # break \n",
    "                continue\n",
    "\n",
    "            # –ü–∞—Ä—Å–∏–Ω–≥\n",
    "            new_count = 0\n",
    "            for article in articles:\n",
    "                job_data = parse_html_card(article)\n",
    "                if job_data:\n",
    "                    all_jobs.append(job_data)\n",
    "                    new_count += 1\n",
    "            \n",
    "            pbar.set_description(f\"Total: {len(all_jobs)}\")\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∂–¥—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü (–Ω–∞ —Å–ª—É—á–∞–π –≤—ã–ª–µ—Ç–∞)\n",
    "            if page % 10 == 0:\n",
    "                pd.DataFrame(all_jobs).to_csv(OUT_CSV, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "\n",
    "            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\n–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_jobs)} –≤–∞–∫–∞–Ω—Å–∏–π.\")\n",
    "        if all_jobs:\n",
    "            df = pd.DataFrame(all_jobs)\n",
    "            df.to_csv(OUT_CSV, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "            print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª: {OUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9410a-f35f-4cc5-ac80-c454a1daff41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
